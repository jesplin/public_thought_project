---
title: "Public Thought Final Project"
author: "Jess Esplin"
date: "11/12/2020"
output: pdf_document
---

```{r}
## INITIAL DATA PROCESSING

# set working directory
library(here)
here()
# setwd("C:/Users/jesse/OneDrive - UW-Madison/2020 Fall Semester/904 AP Field Seminar - Public Opinion/Final Project/ps904-project/public_thought_project")

# load data sets
lvn <- read.csv("data/LVN_data.csv")
tweets <- read.csv("data/tweets_data.csv")
blm3 <- read.csv("data/talk_radio_separate/blm_03.csv")
blm4 <- read.csv("data/talk_radio_separate/blm_04.csv")
blm5 <- read.csv("data/talk_radio_separate/blm_05.csv")
blm6 <- read.csv("data/talk_radio_separate/blm_06.csv")

# merge blm datasets by adding each onto the first as additional rows
talk_radio <- rbind(blm3, blm4, blm5, blm6)

# export combined data set to csv
# write.csv(talk_radio, file = "data/talk_radio.csv")

# moved blm separate csv files into new folder: "data/talk_radio_separate"
```

```{r}
## LOADING DATA GOING FORWARD

# set working directory
# setwd("C:/Users/jesse/OneDrive - UW-Madison/2020 Fall Semester/904 AP Field Seminar - Public Opinion/Final Project/ps904-project/public_thought_project")

# load data sets
lvn <- read.csv("data/LVN_data.csv")
tweets <- read.csv("data/tweets_data.csv")
talk_radio <- read.csv("data/talk_radio.csv")
```

```{r}
# exploring data sets
View(tweets)
# noticed that the tweets start at 0 - probably need to adjust this
tweets[,"body"]

View(talk_radio)
talk_radio[,"content"]
talk_radio[,"context"]

talk_radio[1,"content"]
talk_radio[1,"context"]
# so it looks like content is a small part, maybe a sentence, and context is a few seconds before and after?

unique(talk_radio[,"city"])
#

View(lvn)
# also starts at 0 - will need to adjust this
lvn[,"snips"]
# each snip is part of the conversation in sequence
lvn[,"conv_ID"]
unique(lvn[,"conv_ID"])
# looks like there are 161 conversations

unique(lvn[,"location"])
# looks like there are some issues with the saved locations?? totals to 64 but many combo locations...

# ms: yes there does appear to be an issue with the location...weird
```

```{r}
## Twitter data cleanup & preliminary analysis

# clean up X variable: add 1 so it counts all tweets in data set
tweets$X <- tweets$X+1

# understanding extent of data set
dim(tweets) #46,880 tweets
names(tweets)
head(tweets,10)
tail(tweets,10)
str(tweets) # shows structure of each variable
summary(tweets) # data summary

# look through tweets
tweets[1:10, "body"]
tweets[11:20, "body"]
tweets[21:30, "body"]
tweets[31:40, "body"]
tweets[41:50, "body"] # extent of what I've looked through so far


# try to exclude bots
# though note that it's hard to tell from tweet text if person is real or not, might need more context for that
# examples of bots to exclude:
tweets[14, 2:3] # open letter bot
tweets[85, 2:3] # open letter bot
tweets[389, 2:3] # open letter bot
tweets[14731, 2:3] # resist bot
tweets[12255, 2:3] # NYPD bot
tweets[13805, 2:3] # BLM bot
tweets[4337, 2:3] # NES bot (?)

# whereas these "bots" actually look like real people:
tweets[8693, 2:3] # Lord_Spam_Bot
tweets[41757, 2:3] # KimtheTrollBot

# also seems like most users with "troll" in their username are real...


# what is this??
tweets[29, 2:3] # chain retweets - not sure to include or exclude? not sure if this matters...


# exclude non-English tweets?
tweets[7, 2:3] # spanish


# also exclude news shares??
tweets[40,2:3] # looks like it's just resharing a news article and tagging a bunch of politicians/important handles
tweets[47, 2:3] # shared by "WorldNewsPics" - so maybe need to exclude any handles with "pics" in their username?
tweets[50, 2:3] # not sure about this one either?


# what I'm actually looking for with orgs:
tweets[22, 2:3] # "leading a peaceful protest" "going around my small town" = local organizing/community action
tweets[20, 2:3] # "creating backlash for the community" = local/comm action
tweets[39, 2:3] # maybe this one? refers to protesting in the streets

tweets[49, 2:3] # not sure on this one? "under banner of Black lives Matter"

tweets[31, "body"] # not sure about this one?? references going to a protest and being involved...

```

```{r}
## coding scheme?

# org references:
# "BLM organization"
# "Black KKK"
# "terrorist organization"
# conspiracy theories much?? maybe draw lines there with election fraud; 
# examples: 
tweets[23, 2:3] # "Brics also could have double meaning as actual bricks that was used by the black lives matters protestors to break int into stores..."
tweets[12, 2:3] # Black lives matter ... it is a form of brainwashing used by African-Americans to continue to get their own way in society..."

# i also think there's not much reference to local orgs bc people often tweet to a national/not local audience?

# seeing a lot of fighting over blm vs alm language - maybe mention or incorporate this? I'm not sure it's related enough to orgs to mention with my RQ but could point this out in the data
# examples:
tweets[32, 2:3]
tweets[33, 2:3]
tweets[34, 2:3]
tweets[37, 2:3]
tweets[41, 2:3]

# also description of protests vs riots
tweets[46, 2:3]
```

```{r}
# word cloud of tweets

# install packages
install.packages("tm")  # for text mining
install.packages("SnowballC") # for text stemming
install.packages("wordcloud") # word-cloud generator 
install.packages("RColorBrewer") # color palettes

# load packages
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("tidyverse")

# using text file with all tweets (as of this first run)
text <- readLines("data/tweet-text-file.txt")
# put the text file into a dataframe so it's easier to work with
tweets <- as.data.frame(text)
# omg these are a lot of tweets. let's randomly sample 1,000:
tweets <- tweets[sample(nrow(tweets), 1000), ]
tweets <- as.data.frame(tweets)
names(tweets)
#Create a vector containing only the text
tweetstext <- tweets$tweets
tweetstext <- as.data.frame(tweetstext)
names(tweetstext)
# remove weird characters
tweetstext$text <- gsub(" +"," ",gsub("^ +","",gsub("[^a-zA-Z0-9 ]","", tweetstext$tweetstext)))
# Create a corpus  
corpus <- Corpus(VectorSource(as.vector(tweetstext$text)))

# clean text data via corpus
# ms: there's an error that says "transformation drops documents"--it's just a warning...if you're curious as to what's going on here, it's a check in the underlying code...
summary(corpus)
# remove punctuation
corpus <- tm_map(corpus, content_transformer(removePunctuation))
# to lower
corpus <- tm_map(corpus, content_transformer(tolower))
# remove stopwords
corpus <- tm_map(corpus, content_transformer(removeWords), 
                 stopwords("english"))
# stemming
corpus <- tm_map(corpus, stemDocument)
# remove whitespace
corpus <- tm_map(corpus, stripWhitespace) 
# remove numbers
corpus <- tm_map(corpus, content_transformer(removeNumbers))
# remove some common words
corpus <- tm_map(corpus, removeWords, c("black", "lives", "matter", "blm", "blacklivesmatter", "like", "amp")) 

# create dtm
dtm <- TermDocumentMatrix(corpus) 
matrix <- as.matrix(dtm) 
words <- sort(rowSums(matrix),decreasing=TRUE) 
df <- data.frame(word = names(words),freq=words)

# create word cloud
set.seed(1234) # for reproducibility 
wordcloud(words = df$word, freq = df$freq, min.freq = 1, max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
```
